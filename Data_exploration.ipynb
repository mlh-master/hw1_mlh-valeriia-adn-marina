{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2 - Data exploration & preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's tutorial we will focus on a core skill of data science: data exploration and preprocessing. \\\n",
    "The most important thing to do before any machine learning task is to have a look at your dataset. After doing that, you should be able to answer the following questions:\n",
    "* Does your dataset values make sense to you? \n",
    "* How many missing values does your dataset contain? \n",
    "* What are the relevant variables are there to your task? \n",
    "* Does your data include correlated variables?\n",
    "* Is your data \"clean\"? Should you filter it? \n",
    "* Should you reorganize it in order to have an easy access to it later on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most widely used packages in Python is called `pandas`.  This tutorial will cover will cover some of its' well known commands. Later on, we will also use what one might consider as the most useful package in the field of basic machine learning task which is `scikit-learn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go on, make sure you have updated our environment `bm-336546` with `tutorial2.yml` file as explained in the previous tutorial. Once you are all set we can move on.\n",
    "```shell\n",
    "conda env update --name bm-336546 --file tutorial2.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interfacing with Pycharm\n",
    "Before we continue with this tutorial, we should all become familiar with one of the best Python IDEs which is `PyCharm`. In `PyCharm`, we can debug our code and create a projects that contains many `.py` files that run with the same virtual environment. `PyCharm` also helps us build our code in the correct structure and it even has a spelling check for your comments :). Furthermore, `PyCharm` has the ability to interact with distant servers. \n",
    "\n",
    "In general, the professional version of `PyCharm` has a `Jupyter` [editor](https://www.jetbrains.com/help/pycharm/jupyter-notebook-support.html#ui). Here, however, we will see how to convert `ipynb` files to `.py` files. This can be done easily by a package installed in your `bm-336546` environment called `jupytext`. The `jupytext` package can also convert `.py` files back to `.ipynb` once edited by `PyCharm`. \n",
    "\n",
    "Use Anaconda prompt and `cd` to the correct location. Now you can perform one of the three operations:\n",
    "```shell\n",
    "jupytext --to py notebook.ipynb                 # convert notebook.ipynb to a .py file\n",
    "\n",
    "jupytext --to notebook notebook.py              # convert notebook.py to an .ipynb file with no outputs\n",
    "\n",
    "jupytext --to notebook --execute notebook.py    # convert notebook.py to an .ipynb file and run it \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical topic\n",
    "Diabetes Mellitus affects hundreds of millions of people around the world and can cause many complications if not diagnosed early and treated properly. Diabetes can be predicted ahead using some medical explanatory variables. In our case we will use the study of Pima Indian population near Phoenix, Arizona. All of the patients were women above the age of 21. The population has been under continuous study since 1965 by the National Institute of Diabetes and Digestive and Kidney Diseases because of its high incidence rate of diabetes. In this tutorial we would only focus on the data exploration part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The following features have been provided to help us predict whether a person is diabetic or not:\n",
    "* Pregnancies: Number of times each woman was pregnant.\n",
    "* Glucose: Plasma glucose concentration over 2 hours in an oral glucose tolerance test $[mg/dl]$.\n",
    "* BloodPressure: Diastolic blood pressure in $[mm/Hg]$.\n",
    "* SkinThickness: Triceps skin fold thickness in $[mm]$.\n",
    "* Insulin: Insulin serum over 2 hours in an oral glucose tolerance test $[\\mu U/ml]$.\n",
    "* BMI: Body mass index in $[ kg / m ^ 2 ]$.\n",
    "* DiabetesPedigreeFunction: A function which scores likelihood of diabetes based on family history.\n",
    "* Age: Age in $[years]$.\n",
    "* Outcome: Class variable (0 if non-diabetic, 1 if diabetic).\n",
    "\n",
    "Credit: The data was imported from [Kaggle](https://www.kaggle.com/uciml/pima-indians-diabetes-database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "from sklearn.impute import SimpleImputer \n",
    "from pandas.plotting import scatter_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` can load many types of files into some kind of a table that is called a `DataFrame`. Every column within this table is called a `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'Skin Thickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "df = pd.read_csv(\"Data/PimaDiabetes.csv\", header=None, names=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the most useful commands using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df.info() # general information on data samples amount and their type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()  # print the 5 first observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 5 repetitive random observations using sample and random_state\n",
    "df.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() # print summary statistics of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Question:***</span> *In which cases do we only need the **mean** and the **std** for distribution estimation and in which cases do we need the whole **summary statistics**?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of subsetting dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1:5] # Notice it does not include the last element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = df[['Glucose', 'Insulin']]  # double brackets for column access (i.e. G is also a dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G['Glucose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Glucose', 'Insulin']][1:5]  # Double brackets for column access and additional outer brackets for observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1:5, ['Glucose', 'Insulin']] # loc method allows indexing with string within variables (here it does include the last element!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1:5, 1:3]  # iloc method allows indexing with integers within variables (does not include the last element!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loc` and `iloc` should be used carefully. Basically, `loc` uses strings for columns and labels of rows and `iloc` uses indices. For more information, follow the documentations [here](https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-6fcd0170be9c) and [here](https://datacarpentry.org/python-ecology-lesson/03-index-slice-subset/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now like to examine the distribution of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "axarr = df.hist(bins=50, figsize=(20, 15))  # histograms of dataframe variables\n",
    "xlbl = ['# of pregnancies [N.u]', 'Glucose [mg/dl]', 'Blood Pressure [mm/Hg]','Skin Thickness [mm]', \n",
    "        'Insulin [uU/ml]','BMI [Kg/m^2]','DPF [N.U]', 'Age [years]', 'Diabetes [binary]' ]\n",
    "\n",
    "for idx, ax in enumerate(axarr.flatten()):\n",
    "    ax.set_xlabel(xlbl[idx])\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from those histograms that some of the variables are impossible such as 0 values in BMI, insulin, skin thickness and blood pressure.\\\n",
    "First, we'll replace these values with nan. \n",
    "\n",
    "*All of the operations will be applied on a copy of the dataframe called* `df_nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan = df.copy()\n",
    "df_nan.loc[:, 'Glucose':'BMI'] = df_nan.loc[:, 'Glucose':'BMI'].replace(0, np.nan) # replace the non-realistic (0 in our case) values with nan\n",
    "df_nan.isna().sum()/df.shape[0] # fraction of replaced values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the missing data are in the variables *insulin* and *skin thickness*. There are several ways to handle missing values. Here are some examples:\n",
    "* The variable's missing values can be imputed by some value (median for instance).\n",
    "* Can replaced by randomly picked values from the rest of the data's distribution.\n",
    "* The probability density function can be estimated from the variable's values histogram and missing values would be replaced by sampled values from the pdf.\n",
    "* The missing values can be replaced by random values from the variable's values.\n",
    "* The total variable can be eliminated when there is no sufficient number of samples.\n",
    "\n",
    "For more options, visit [this site](https://hrngok.github.io/posts/missing%20values/).\n",
    "\n",
    "Here, we will show only median imputation in two different methods within the relevant variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan.loc[:, 'Glucose':'BMI'] = df_nan.loc[:, 'Glucose':'BMI'].fillna(df_nan.median())  # method 1\n",
    "df_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df_nan again for second method\n",
    "df_nan = df.copy()\n",
    "df_nan.loc[:, 'Glucose':'BMI'] = df_nan.loc[:, 'Glucose':'BMI'].replace(0, np.nan) # replace the non-realistic results with nan\n",
    "df_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\") # method 2, mostly preferred due to it's generalized form\n",
    "p = imputer.fit(df_nan)\n",
    "X = imputer.transform(df_nan)\n",
    "df1 = pd.DataFrame(X, columns=df_nan.columns)  # construct X object as Dataframe\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axarr = df1.hist(bins=50, figsize=(20, 15)) # histograms of dataframe variables\n",
    "for idx, ax in enumerate(axarr.flatten()):\n",
    "    ax.set_xlabel(xlbl[idx])\n",
    "    ax.set_ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the median imputation did not \"work well\" for *insulin* and *skin thickness* due to multiple missing values. \n",
    "\n",
    "- For the *insulin* values, it might be better to replace them with values drawn from the distribution which has a pretty low variance relative to the mean. \n",
    "\n",
    "- For the *skin thickness variable*, we can consider elimination of all the variable's values if we assume that it does not affect the outcome. \n",
    "\n",
    "Either way, it is not right to just \"drop\" the missing samples of both variables because it will significantly reduce the amount of data but it is reasonable to \"drop\" a feature. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's move forward and see how to apply a function on a `dataframe` variable. In our case we will replace `nan` with random values distributed as the current value distribution.\n",
    "\n",
    "In order to do so, we will now apply median imputation on all of the variables which are not *insulin* or *skin thickness*. \\\n",
    "We will then apply random sampling on *insulin* variable values and \"drop\" the \"skin thickness\" variable. All of the operations will now be applied directly on the original `dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'Glucose':'BMI'] = df.loc[:, 'Glucose':'BMI'].replace(0, np.nan) # replace the non-realistic results with nan\n",
    "df.loc[:, ['Glucose', 'BloodPressure', 'BMI']] = df.loc[:, ['Glucose', 'BloodPressure', 'BMI']].fillna(df.median())  # apply a \"known funtion\" on selected variables # median imputation\n",
    "df.drop(columns=['Skin Thickness'],inplace=True)\n",
    "insulin_hist = df_nan.loc[:,'Insulin'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_sampling(x, var_hist):\n",
    "    if x == np.nan:\n",
    "        rand_idx = np.random.choice(len(var_hist))\n",
    "        x = var_hist[rand_idx]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Insulin']].applymap(lambda x: rand_sampling(x, insulin_hist))\n",
    "xlbl.remove('Skin Thickness [mm]')\n",
    "xlbl.append('')\n",
    "\n",
    "axarr = df.hist(bins=50, figsize=(20, 15))\n",
    "for idx, ax in enumerate(axarr.flatten()):\n",
    "    ax.set_xlabel(xlbl[idx])\n",
    "    ax.set_ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to the missing category and the difference in the insulin figure.\n",
    "\n",
    "In many tasks, we may find that we need to to scale our data. Each task will likely require a specific kind of scaling. \\\n",
    "The scaling process will help us to correctly identify the variables that are most important for our task regardless their magnitudes. \n",
    "\n",
    "Here is an example of scaling your data using the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = preprocessing.StandardScaler().fit_transform(df.values)\n",
    "scaled_features_df = pd.DataFrame(scaled_features, index=df.index, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features_df.hist(bins=50, figsize=(20, 15)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Question:***</span> *Should we scale all of our data as we did?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we can find correlations among selected variables. \\\n",
    "This can help us later on in choosing the most relevant variables with minimum redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"Age\", \"BMI\", \"Glucose\", \"Pregnancies\"]\n",
    "scatter_matrix(df[attributes], figsize=(12, 8)) # correlation between chosen variables\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we can't really find any significant correlation except age and pregnancies which was pretty obvious to begin with. \n",
    "\n",
    "Another important thing that we would like to check within our data is the prevalence. \\\n",
    "Let's check what is the prevalence of diabetes in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_diab = 100 * df['Outcome'].value_counts(normalize=True)  # normalize=True for percentage\n",
    "\n",
    "print(r'%.2f%% of the Pima tribe women have diabetes.' % prc_diab[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we would like to count values above or below a specific threshold to get a sense of the data. Then, we can check if those conditions have any impact on the outcome prevalence, or in other words, check if they are *predicative*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = df[df['Glucose'] > 150].shape[0]  # how many of the tribe women have glucose values higher than 150\n",
    "\n",
    "print(r'%d women have glucose values higher than 150 [mg/dl].' % val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_obs = df[(df['Glucose'] > 150) & (df['Insulin'] > 100)]  # Extract patients who have glucose values higher than 150 and insulin values higher than 100\n",
    "val = 100 * selected_obs['Outcome'].value_counts(normalize=True)[1]  # show how many of the selected patients have diabetes.\n",
    "\n",
    "print(r'Out of the women who have glucose values higher than 150[mg/dl] and insulin values higher than 100[uU/ml], %.2f%% have diabetes.' % val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant deviation can be seen in the prevalence once we choose women with high levels of insulin and glucose. \n",
    "\n",
    "\n",
    "The last things that we will see in this tutorial is how to group, sort, filter and plot variables. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='Pregnancies') # Notice the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Pregnancies').describe()  # summary statistics of subsets of women who had the same number of pregnancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Pregnancies').describe()['Age'] # for a single variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preg_group = df.groupby('Pregnancies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preg_group.get_group(5)['Age'].shape  # how many women have had 5 pregnancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preg_group.filter(lambda x: len(x) > 24) # drop groups who have less than 24 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot('Age', 'Glucose', kind='scatter') # scatter plot of two variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "> ### <span style=\"color:#3385ff\">In this tutorial we demonstrated some of the capabilities of `pandas`. We are sure that you will find the capabilities of `pandas` very useful in almost every task in data science. *See you next time!*</span>\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *This tutorial was written by Moran Davoodi & Alon Begin with the assitance of Yuval Ben Sason & Kevin Kotzen*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
